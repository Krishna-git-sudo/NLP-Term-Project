{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBzdXiYHpyxP",
        "outputId": "d508ce69-f079-43a1-c3f3-966cc6790b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login with your token\n",
        "login(token=\"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\")\n",
        "print(\"Successfully logged in!\")"
      ],
      "metadata": {
        "id": "i3BvGZvYgcNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3c3c4c-6790-4165-b118-d28cda69645d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi74nEGlIEiN"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade trl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukgGHSy8z0x0"
      },
      "outputs": [],
      "source": [
        "#!pip install -q accelerate==1.1.1 peft==0.13.2 bitsandbytes==0.41.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "bOFDoB5-B20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "2f4789a1f0a940af98fbf420e647d91a",
            "0c7ab0f46c654403b12022317a47b11d",
            "512ed9a66608465cacd6827532ca3f43",
            "a103157392d9444f898ea68b9122ebcb",
            "8e04da1d938448e685d3f0dc046b9e10",
            "dd065203118a4fe894ba3443bf0854ea",
            "c84cf4eacfcf4bf39e3eb8aafb7df1db",
            "830ff35654b04445a3f4fb6bb1a28175",
            "66ab855b429845898345a0c7d1802c8e",
            "c69993746bc34d26a411ada9f1702d19",
            "baab901383984bc89acb6a9d963c53b1",
            "c466844da02540b1a1b8074f51c5aa09",
            "493eaf44b38a48e1b2b9a92cc82c859b",
            "f4ab6be0ac5d4a22953742e0cc9d82f9",
            "0ca2b0def97744bdaf9b9fe1e96e154f",
            "1a93e2562b2441589881f544fc620bef",
            "5c34fe51b03a4f2582684e0470f13fc0",
            "cfacdf1c2c264466926d5119b35abd93",
            "bce5d03f5bab4b19a3fda9fb0e3bbbe1",
            "a3528dc5228f4c589e8701c7c1b525e2",
            "ac3a1bd28f9441e7aebc66b9dca6d8e4",
            "397c9bb5b70e474d8c24ddb6414314c6"
          ]
        },
        "id": "qn_a7Y2x2YYm",
        "outputId": "8a0ef40f-795d-4d49-c919-35069a78526c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f4789a1f0a940af98fbf420e647d91a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c466844da02540b1a1b8074f51c5aa09"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Define the model name and dataset\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"  # General Llama-2 model\n",
        "dataset_name = \"squad\"  # SQuAD dataset\n",
        "new_model = \"Llama-3.2-1B-qa-finetuned\"  # Name for the fine-tuned model\n",
        "\n",
        "# QLoRA Parameters for fine-tuning\n",
        "lora_r = 32\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# BitsAndBytes Parameters for 4-bit precision\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "\n",
        "# Training arguments\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 2\n",
        "per_device_train_batch_size = 1\n",
        "gradient_accumulation_steps = 4\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 0\n",
        "logging_steps = 25\n",
        "evaluation_strategy = \"epoch\"\n",
        "save_total_limit = 2\n",
        "load_best_model_at_end = True\n",
        "\n",
        "# Load the SQuAD dataset and preprocess for QA\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "# def preprocess_qa(example):\n",
        "#     \"\"\"\n",
        "#     Preprocesses a QA example into a format suitable for training:\n",
        "#     - Combine context and question into input\n",
        "#     - Use the answer as the expected output\n",
        "#     \"\"\"\n",
        "#     question = example[\"question\"]\n",
        "#     context = example[\"context\"]\n",
        "#     answers = example[\"answers\"][\"text\"]\n",
        "#     answer = answers[0] if answers else \"\"  # Use the first answer\n",
        "#     input_text = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "#     return {\"input_text\": input_text, \"label\": answer}\n",
        "\n",
        "def preprocess_qa(example):\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = example[\"answers\"][\"text\"]\n",
        "    answer = answers[0] if answers else \"\"  # Take the first answer (or handle multiple answers differently)\n",
        "    input_text = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    return {\"input_text\": input_text, \"label\": answer}\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_dataset = dataset.map(preprocess_qa, batched=False)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# def tokenize_qa(examples):\n",
        "#     \"\"\"\n",
        "#     Tokenizes the preprocessed examples, padding them to ensure uniform input sizes.\n",
        "#     \"\"\"\n",
        "#     inputs = tokenizer(examples[\"input_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "#     outputs = tokenizer(examples[\"label\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     # Ensuring labels have the same length as inputs\n",
        "#     inputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "\n",
        "#     # Add padding to input_ids and labels if necessary\n",
        "#     max_length = max(len(inputs[\"input_ids\"]), len(inputs[\"labels\"]))\n",
        "#     inputs[\"input_ids\"] = inputs[\"input_ids\"] + [tokenizer.pad_token_id] * (max_length - len(inputs[\"input_ids\"]))\n",
        "#     inputs[\"labels\"] = inputs[\"labels\"] + [tokenizer.pad_token_id] * (max_length - len(inputs[\"labels\"]))\n",
        "\n",
        "#     return inputs\n",
        "\n",
        "def tokenize_qa(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    outputs = tokenizer(examples[\"label\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize train and validation splits\n",
        "train_dataset = processed_dataset[\"train\"].map(tokenize_qa, batched=True)\n",
        "validation_dataset = processed_dataset[\"validation\"].map(tokenize_qa, batched=True)\n",
        "\n",
        "# ---------------------------------  Take a random subset ---------------------------------------\n",
        "train_dataset = train_dataset.shuffle(seed=42)\n",
        "validation_dataset = validation_dataset.shuffle(seed=42)\n",
        "\n",
        "# If there is no 'test' split, create it from the 'train' dataset\n",
        "test_dataset = train_dataset.shuffle(seed=42)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "validation_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "3nnyuIC_DN2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sample(dataset, dataset_name, tokenizer):\n",
        "    print(f\"\\nSample data from the {dataset_name} dataset:\")\n",
        "    for i in range(3):  # Print first 3 samples\n",
        "        input_ids = dataset[i]['input_ids']\n",
        "        labels = dataset[i]['labels']\n",
        "\n",
        "        # Decode the tokenized input and labels to actual text\n",
        "        input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "        label_text = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Input Text: {input_text}\")\n",
        "        print(f\"Labels: {label_text}\")\n",
        "        print()\n",
        "\n",
        "# Example usage:\n",
        "print_sample(train_dataset, \"train\", tokenizer)\n",
        "print_sample(validation_dataset, \"validation\", tokenizer)\n",
        "print_sample(test_dataset, \"test\", tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soOB9AdXCbo7",
        "outputId": "e2dabbf1-e53a-4df1-fb1e-7f1a54352ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample data from the train dataset:\n",
            "Sample 1:\n",
            "Input Text: Context: The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerated by the government. According to a 2010 Pew Global Attitudes survey, 84% of Egyptians polled supported the death penalty for those who leave Islam; 77% supported whippings and cutting off of hands for theft and robbery; and 82% support stoning a\n",
            "Labels: 84%\n",
            "\n",
            "Sample 2:\n",
            "Input Text: Context: The Ann Arbor Hands-On Museum is located in a renovated and expanded historic downtown fire station. Multiple art galleries exist in the city, notably in the downtown area and around the University of Michigan campus. Aside from a large restaurant scene in the Main Street, South State Street, and South University Avenue areas, Ann Arbor ranks first among U.S. cities in the number of booksellers and books sold per capita. The Ann Arbor District Library maintains four branch outlets in addition to its main downtown building. The city is also home to the Gerald R. Ford Presidential Library.\n",
            "Question: Ann Arbor ranks 1st among what goods sold?\n",
            "\n",
            "Labels: books\n",
            "\n",
            "Sample 3:\n",
            "Input Text: Context: One important aspect of the rule-of-law initiatives is the study and analysis of the rule of law’s impact on economic development. The rule-of-law movement cannot be fully successful in transitional and developing countries without an answer to the question: does the rule of law matter for economic development or not? Constitutional economics is the study of the compatibility of economic and financial decisions within existing constitutional law frameworks, and such a framework includes government spending on the judiciary, which, in many transitional and developing countries, is completely controlled by the executive. It is useful to distinguish between the two methods of corruption of the judiciary: corruption by the executive branch,\n",
            "Labels: the executive\n",
            "\n",
            "\n",
            "Sample data from the validation dataset:\n",
            "Sample 1:\n",
            "Input Text: Context: Private schooling in the United States has been debated by educators, lawmakers and parents, since the beginnings of compulsory education in Massachusetts in 1852. The Supreme Court precedent appears to favor educational choice, so long as states may set standards for educational accomplishment. Some of the most relevant Supreme Court case law on this is as follows: Runyon v. McCrary, 427 U.S. 160 (1976); Wisconsin v. Yoder, 406 U.S. 205 (1972); Pierce v. Society of Sisters, 268 U.S. 510 (1925); Meyer v. Nebraska, 262\n",
            "Labels: 1852\n",
            "\n",
            "Sample 2:\n",
            "Input Text: Context: The chloroplast membranes sometimes protrude out into the cytoplasm, forming a stromule, or stroma-containing tubule. Stromules are very rare in chloroplasts, and are much more common in other plastids like chromoplasts and amyloplasts in petals and roots, respectively. They may exist to increase the chloroplast's surface area for cross-membrane transport, because they are often branched and tangled with the endoplasmic reticulum. When they were first observed in 1962, some plant biologists dismissed the structures as artifactual, claiming that stromules were just\n",
            "Labels: 1962\n",
            "\n",
            "Sample 3:\n",
            "Input Text: Context: Not only the work of British artists and craftspeople is on display, but also work produced by European artists that was purchased or commissioned by British patrons, as well as imports from Asia, including porcelain, cloth and wallpaper. Designers and artists whose work is on display in the galleries include Gian Lorenzo Bernini, Grinling Gibbons, Daniel Marot, Louis Laguerre, Antonio Verrio, Sir James Thornhill, William Kent, Robert Adam, Josiah Wedgwood, Matthew Boulton, Canova, Thomas Chippendale, Pugin, William Morris. Patrons who have influenced taste are also\n",
            "Labels: Horace Walpole\n",
            "\n",
            "\n",
            "Sample data from the test dataset:\n",
            "Sample 1:\n",
            "Input Text: Context: The iTunes Store (introduced April 29, 2003) is an online media store run by Apple and accessed through iTunes. The store became the market leader soon after its launch and Apple announced the sale of videos through the store on October 12, 2005. Full-length movies became available on September 12, 2006.\n",
            "Question: When were videos made available through the iTunes store?\n",
            "Answer:\n",
            "Labels: October 12, 2005\n",
            "\n",
            "Sample 2:\n",
            "Input Text: Context: In April 2013, Marvel and other Disney conglomerate components began announcing joint projects. With ABC, a Once Upon a Time graphic novel was announced for publication in September. With Disney, Marvel announced in October 2013 that in January 2014 it would release its first title under their joint \"Disney Kingdoms\" imprint \"Seekers of the Weird\", a five-issue miniseries. On January 3, 2014, fellow Disney subsidiary Lucasfilm Limited, LLC announced that as of 2015, Star Wars comics would once again be published by Marvel.\n",
            "Question: What series on this network was given\n",
            "Labels: Once Upon a Time\n",
            "\n",
            "Sample 3:\n",
            "Input Text: Context: The Defence Committee—Third Report \"Defence Equipment 2009\" cites an article from the Financial Times website stating that the Chief of Defence Materiel, General Sir Kevin O’Donoghue, had instructed staff within Defence Equipment and Support (DE&S) through an internal memorandum to reprioritize the approvals process to focus on supporting current operations over the next three years; deterrence related programmes; those that reflect defence obligations both contractual or international; and those where production contracts are already signed. The report also cites concerns over potential cuts in the defence science and technology research budget; implications of inappropriate estimation of Defence Inflation within\n",
            "Labels: General Sir Kevin O’Donoghue\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some samples from the train, validation, and test datasets\n",
        "def print_sample(dataset, dataset_name):\n",
        "    print(f\"\\nSample data from the {dataset_name} dataset:\")\n",
        "    for i in range(3):  # Print first 3 samples\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"Input IDs: {dataset[i]['input_ids']}\")\n",
        "        print(f\"Labels: {dataset[i]['labels']}\")\n",
        "        print()\n",
        "\n",
        "# Print samples from each dataset\n",
        "print_sample(train_dataset, \"train\")\n",
        "print_sample(validation_dataset, \"validation\")\n",
        "print_sample(test_dataset, \"test\")"
      ],
      "metadata": {
        "id": "6ely5p7SOZAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ce23b5-c4e7-466b-9b8d-f9a745610763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample data from the train dataset:\n",
            "Sample 1:\n",
            "Input IDs: tensor([128000,   2014,     25,    578,  57717,  17997,    389,  44193,    612,\n",
            "          3142,   9601,  21467,  15212,    439,    279,  18172,  12047,   3224,\n",
            "           304,    279,   1917,    369,  10597,  11542,     13,    578,   3723,\n",
            "          4273,   9849,    389,   7327,  53176,  25320,     11,    264,  52008,\n",
            "          9678,   9266,    315,    279,   2326,   3109,     11,    706,   9277,\n",
            "         15212,    389,   1202,   3821,   1160,    315,   5961,    430,   1397,\n",
            "          3345,  16967,   4245,    311,    279,   7138,    323,  13112,    315,\n",
            "         27655,    315,  10597,  11542,  17045,    304,    477,  66441,    555,\n",
            "           279,   3109,     13,  10771,    311,    264,    220,    679,     15,\n",
            "         57717,   8121,   7867,  21237,  10795,     11,    220,   5833,      4,\n",
            "           315,  82604,  84721,   7396,    279,   4648,  16750,    369,   1884,\n",
            "           889,   5387,  15256,     26,    220,   2813,      4,   7396,    421,\n",
            "          2877,    826,    323,  14713,   1022,    315,   6206,    369,  28483,\n",
            "           323,  44214,     26,    323,    220,   6086,      4,   1862,    357,\n",
            "         20324,    264])\n",
            "Labels: tensor([128000,   5833,      4, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 2:\n",
            "Input IDs: tensor([128000,   2014,     25,    578,   9489,  65639,  43396,  67454,  16730,\n",
            "           374,   7559,    304,    264,  57934,    323,  17626,  18526,  19441,\n",
            "          4027,   8216,     13,  29911,   1989,  43654,   3073,    304,    279,\n",
            "          3363,     11,  35146,    304,    279,  19441,   3158,    323,   2212,\n",
            "           279,   3907,    315,  14972,  15679,     13,  57194,    505,    264,\n",
            "          3544,  10960,   6237,    304,    279,   4802,   6825,     11,   4987,\n",
            "          3314,   6825,     11,    323,   4987,   3907,  17569,   5789,     11,\n",
            "          9489,  65639,  21467,   1176,   4315,    549,    815,     13,   9919,\n",
            "           304,    279,   1396,    315,   6603,  25812,    323,   6603,   6216,\n",
            "           824,  53155,     13,    578,   9489,  65639,  11182,  11896,  33095,\n",
            "          3116,   9046,  28183,    304,   5369,    311,   1202,   1925,  19441,\n",
            "          4857,     13,    578,   3363,    374,   1101,   2162,    311,    279,\n",
            "         55357,    432,     13,  14337,  42855,  11896,    627,  14924,     25,\n",
            "          9489,  65639,  21467,    220,     16,    267,   4315,   1148,  11822,\n",
            "          6216,   5380])\n",
            "Labels: tensor([128000,  12383, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 3:\n",
            "Input IDs: tensor([128000,   2014,     25,   3861,   3062,  13189,    315,    279,   6037,\n",
            "          8838,  31412,  28271,    374,    279,   4007,    323,   6492,    315,\n",
            "           279,   6037,    315,   2383,    753,   5536,    389,   7100,   4500,\n",
            "            13,    578,   6037,   8838,  31412,   7351,   4250,    387,   7373,\n",
            "          6992,    304,  66743,    323,  11469,   5961,   2085,    459,   4320,\n",
            "           311,    279,   3488,     25,   1587,    279,   6037,    315,   2383,\n",
            "          5030,    369,   7100,   4500,    477,    539,     30,  63285,  28989,\n",
            "           374,    279,   4007,    315,    279,  25780,    315,   7100,    323,\n",
            "          6020,  11429,   2949,   6484,  25543,   2383,  49125,     11,    323,\n",
            "          1778,    264,  12914,   5764,   3109,  10374,    389,    279,  72975,\n",
            "            11,    902,     11,    304,   1690,  66743,    323,  11469,   5961,\n",
            "            11,    374,   6724,  14400,    555,    279,  11145,     13,   1102,\n",
            "           374,   5505,    311,  33137,   1990,    279,   1403,   5528,    315,\n",
            "         21948,    315,    279,  72975,     25,  21948,    555,    279,  11145,\n",
            "          9046,     11])\n",
            "Labels: tensor([128000,   1820,  11145, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "\n",
            "Sample data from the validation dataset:\n",
            "Sample 1:\n",
            "Input IDs: tensor([128000,   2014,     25,   9877,  68721,    304,    279,   3723,   4273,\n",
            "           706,   1027,  59674,    555,  50794,     11,  26137,    323,   6699,\n",
            "            11,   2533,    279,  67482,    315,  65868,   6873,    304,  22108,\n",
            "           304,    220,   9741,     17,     13,    578,  13814,   7301,  47891,\n",
            "          8111,    311,   4799,  16627,   5873,     11,    779,   1317,    439,\n",
            "          5415,   1253,    743,  10886,    369,  16627,  61238,     13,   4427,\n",
            "           315,    279,   1455,   9959,  13814,   7301,   1162,   2383,    389,\n",
            "           420,    374,    439,  11263,     25,   6588,  26039,    348,     13,\n",
            "         14583,   3535,     11,    220,  20465,    549,    815,     13,    220,\n",
            "          6330,    320,   4468,     21,   1237,  21073,    348,     13,    816,\n",
            "          4414,     11,    220,  17264,    549,    815,     13,    220,  10866,\n",
            "           320,   4468,     17,   1237,  50930,    348,     13,  13581,    315,\n",
            "         62088,     11,    220,  16332,    549,    815,     13,    220,  15633,\n",
            "           320,   5926,     20,   1237,  48290,    348,     13,  38379,     11,\n",
            "           220,  14274])\n",
            "Labels: tensor([128000,   9741,     17, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 2:\n",
            "Input IDs: tensor([128000,   2014,     25,    578,  37833,  92335,  79348,   7170,  81458,\n",
            "           799,    704,   1139,    279,   9693,  99705,  10753,     11,  30164,\n",
            "           264, 120004,   1130,     11,    477,    357,  58084,  93871,  15286,\n",
            "          1130,     13,  93703,   2482,    527,   1633,   9024,    304,  37833,\n",
            "         92335,     82,     11,    323,    527,   1790,    810,   4279,    304,\n",
            "          1023,  88247,   3447,   1093,  22083,  92335,     82,    323,  64383,\n",
            "           385,    501,  12019,    304,  96740,    323,  20282,     11,  15947,\n",
            "            13,   2435,   1253,   3073,    311,   5376,    279,  37833,  92335,\n",
            "           596,   7479,   3158,    369,   5425,   1474,  60163,   2194,   7710,\n",
            "            11,   1606,    814,    527,   3629,  53358,   2454,    323,  93941,\n",
            "           449,    279,    842,  56178,  10753,    292,   2160,    292,  16903,\n",
            "            13,   3277,    814,   1051,   1176,  13468,    304,    220,   5162,\n",
            "            17,     11,   1063,   6136,   6160,  22012,  27292,    279,  14726,\n",
            "           439,   1989,    333,  12210,     11,  21039,    430, 120004,   2482,\n",
            "          1051,   1120])\n",
            "Labels: tensor([128000,   5162,     17, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 3:\n",
            "Input IDs: tensor([128000,   2014,     25,   2876,   1193,    279,    990,    315,   8013,\n",
            "         13820,    323,  44948,  16455,    374,    389,   3113,     11,    719,\n",
            "          1101,    990,   9124,    555,   7665,  13820,    430,    574,  15075,\n",
            "           477,  44224,    555,   8013,  52860,     11,    439,   1664,    439,\n",
            "         15557,    505,  13936,     11,   2737,  78742,     11,  28392,    323,\n",
            "         44686,     13,   7127,    388,    323,  13820,   6832,    990,    374,\n",
            "           389,   3113,    304,    279,  43654,   2997,  79704,  74500,  14502,\n",
            "          6729,     11,   2895,    258,   2785,  29479,  47620,     11,  15469,\n",
            "          2947,    354,     11,  12140,  33471,   8977,    265,     11,  23245,\n",
            "          6383,  10599,     11,  17177,   7957,  78024,  28607,     11,  12656,\n",
            "         18206,     11,   8563,  15387,     11,  28978,  19870,   6658,     70,\n",
            "          6798,     11,  19475,    426,  11206,    783,     11,   3053,  12949,\n",
            "            11,  11355,    921,   2877,  70160,     11,    393,   3715,     11,\n",
            "         12656,  30283,     13,   7281,  26692,    889,    617,  28160,  12945,\n",
            "           527,   1101])\n",
            "Labels: tensor([128000,  40701,    580,  14916,  69172, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "\n",
            "Sample data from the test dataset:\n",
            "Sample 1:\n",
            "Input IDs: tensor([128000,   2014,     25,    578,  13323,   9307,    320,    396,  30317,\n",
            "          5936,    220,   1682,     11,    220,   1049,     18,      8,    374,\n",
            "           459,   2930,   3772,   3637,   1629,    555,   8325,    323,  25790,\n",
            "          1555,  13323,     13,    578,   3637,   6244,    279,   3157,   7808,\n",
            "          5246,   1306,   1202,   7195,    323,   8325,   7376,    279,   6412,\n",
            "           315,   6946,   1555,    279,   3637,    389,   6664,    220,    717,\n",
            "            11,    220,   1049,     20,     13,   8797,  30425,   9698,   6244,\n",
            "          2561,    389,   6250,    220,    717,     11,    220,   1049,     21,\n",
            "           627,  14924,     25,   3277,   1051,   6946,   1903,   2561,   1555,\n",
            "           279,  13323,   3637,   5380,  16533,     25, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "Labels: tensor([128000,  31294,    220,    717,     11,    220,   1049,     20, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 2:\n",
            "Input IDs: tensor([128000,   2014,     25,    763,   5936,    220,    679,     18,     11,\n",
            "         22883,    323,   1023,  16795,  88751,    349,   6956,   6137,  38787,\n",
            "         10496,   7224,     13,   3161,  19921,     11,    264,   9843,  30538,\n",
            "           264,   4212,  21154,  11775,    574,   7376,    369,  17009,    304,\n",
            "          6250,     13,   3161,  16795,     11,  22883,   7376,    304,   6664,\n",
            "           220,    679,     18,    430,    304,   6186,    220,    679,     19,\n",
            "           433,   1053,   4984,   1202,   1176,   2316,   1234,    872,  10496,\n",
            "           330,  72765,  15422,     82,      1,  79967,    330,  40450,    388,\n",
            "           315,    279,  77253,    498,    264,   4330,  90465,   1332,    285,\n",
            "          4804,     13,   1952,   6186,    220,     18,     11,    220,    679,\n",
            "            19,     11,  12637,  16795,  41164,  32103,  31255,  19439,     11,\n",
            "         15620,   7376,    430,    439,    315,    220,    679,     20,     11,\n",
            "          7834,  15317,  29159,   1053,   3131,   1578,    387,   4756,    555,\n",
            "         22883,    627,  14924,     25,   3639,   4101,    389,    420,   4009,\n",
            "           574,   2728])\n",
            "Labels: tensor([128000,  12805,  30538,    264,   4212, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n",
            "Sample 3:\n",
            "Input IDs: tensor([128000,   2014,     25,    578,  40007,  10554,   2345,  38075,   8423,\n",
            "           330,   2685,    768,  20849,    220,   1049,     24,      1,  58273,\n",
            "           459,   4652,    505,    279,  17961,   8691,   3997,  28898,    430,\n",
            "           279,  14681,    315,  40007,  99408,  13327,     11,   3331,  17177,\n",
            "         16768,    507,    529,   8161,  65036,    361,     11,   1047,  42075,\n",
            "          5687,   2949,  40007,  20849,    323,   9365,    320,   1170,  90385,\n",
            "             8,   1555,    459,   5419,  82758,    311,   2109,   3334,  27406,\n",
            "           279,  83923,   1920,    311,   5357,    389,  12899,   1510,   7677,\n",
            "           927,    279,   1828,   2380,   1667,     26,   4130,  16271,   5552,\n",
            "         38737,     26,   1884,    430,   8881,  23682,  30255,   2225,  76543,\n",
            "           477,   6625,     26,    323,   1884,   1405,   5788,  17517,    527,\n",
            "          2736,   8667,     13,    578,   1934,   1101,  58273,  10742,    927,\n",
            "          4754,  15455,    304,    279,  23682,   8198,    323,   5557,   3495,\n",
            "          8199,     26,  25127,    315,  33781,  42304,    315,  40007,    763,\n",
            "         65249,   2949])\n",
            "Labels: tensor([128000,  15777,  17177,  16768,    507,    529,   8161,  65036,    361,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
            "        128001, 128001])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a DataCollator for padding dynamically during training\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_name, padding=True)\n",
        "\n",
        "# Load the general Llama-2 model with 4-bit precision and LoRA configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=getattr(torch, bnb_4bit_compute_dtype),\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Load the general Llama-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Automatically select the device map\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # Disable cache for training\n",
        "\n",
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    group_by_length=group_by_length,\n",
        "    evaluation_strategy=\"epoch\",  # Use 'eval_strategy' instead of 'evaluation_strategy'\n",
        "    save_strategy=\"epoch\",  # Example strategy\n",
        "    save_total_limit=save_total_limit,\n",
        "    load_best_model_at_end=load_best_model_at_end,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Fine-tune using SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=data_collator,\n",
        "    max_seq_length=128,\n",
        ")\n",
        "trainer.processing_class = tokenizer"
      ],
      "metadata": {
        "id": "bOItlQxXOVuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5ab631-d338-4402-e4d7-d2eeefbfed60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaKhuB--2YUR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "7dff33c5-29fa-4bcc-d2f0-41a2d1f2e230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 35:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.414900</td>\n",
              "      <td>0.358187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.359300</td>\n",
              "      <td>0.352545</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1500, training_loss=0.5762089230219523, metrics={'train_runtime': 2151.0206, 'train_samples_per_second': 2.789, 'train_steps_per_second': 0.697, 'total_flos': 4499960758272000.0, 'train_loss': 0.5762089230219523, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_o5pU972YR2"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "#trainer.save_model(output_dir)\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWvN9ejXb_Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "Up4VrXPAb-_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"/content/Llama-3.2-1B-qa-finetuned\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/Llama-3.2-1B-qa-finetuned\")\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0,max_length=300)\n",
        "\n",
        "\n",
        "# Define the context\n",
        "context = r\"\"\"\n",
        "The Earth orbits the Sun, and this motion occurs along an elliptical path.\n",
        "The gravitational pull between the Earth and the Sun keeps Earth in orbit.\n",
        "This interaction causes the Earth to move in a consistent way around the Sun,\n",
        "taking approximately 365.25 days to complete one orbit, which is what defines a year.\n",
        "\"\"\"\n",
        "\n",
        "# Perform the question-answering task\n",
        "result = question_answerer(question=\"How long does it take for Earth to orbit the Sun?\", context=context)\n",
        "\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n"
      ],
      "metadata": {
        "id": "1wgjWiCIPfsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_NAME = \"/content/alberta_squad\"  # Updated folder name\n",
        "JSON_TEST_FILE = \"/content/test_set.json\"\n",
        "data_path = '/content/alberta_squad/test_set.json'\n",
        "file_path = '/content/alberta_squad/test_set.json'\n",
        "checkpoint_path = data_path"
      ],
      "metadata": {
        "id": "rAH-Pbuup4Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "\n",
        "class LoadData:\n",
        "    def __init__(self,path_to_json_file: str,checkpoint_path: str,train_file: str = 'train.json',val_file: str = 'val.json') -> None:\n",
        "        \"\"\"\n",
        "        Load and process SQuAD data by flattening the JSON structure\n",
        "        and saving it into train and validation JSON files.\n",
        "\n",
        "        Args:\n",
        "            path_to_json_file (str): Path to the input JSON file.\n",
        "            checkpoint_path (str): Directory to save the output JSON files.\n",
        "            train_file (str): Name of the train JSON file to be created.\n",
        "            val_file (str): Name of the validation JSON file to be created.\n",
        "        \"\"\"\n",
        "        self.path_to_json_file = Path(path_to_json_file)\n",
        "        self.checkpoint_path = Path(checkpoint_path)\n",
        "\n",
        "        # Ensure the checkpoint path exists\n",
        "        self.checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.train_file = self.checkpoint_path / train_file\n",
        "        self.val_file = self.checkpoint_path / val_file\n",
        "\n",
        "        self.data = self.load_data()\n",
        "\n",
        "    def load_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Load the JSON file, flatten the data, and save train/validation files.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with self.path_to_json_file.open('r', encoding='utf-8') as f:\n",
        "                train_data = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"JSON file not found at {self.path_to_json_file}\")\n",
        "        except json.JSONDecodeError:\n",
        "            raise ValueError(f\"Error decoding JSON file at {self.path_to_json_file}\")\n",
        "\n",
        "        print(f'Flattening SQuAD {train_data.get(\"version\", \"unknown\")} data...')\n",
        "\n",
        "        train_data_flat, val_data_flat, errors = self.load_squad_data(train_data)\n",
        "\n",
        "        print(f'\\nErroneous Datapoints: {errors}')\n",
        "        self._save_json(self.train_file, {'data': train_data_flat})\n",
        "        self._save_json(self.val_file, {'data': val_data_flat})\n",
        "\n",
        "    def load_squad_data(self, data: Dict, split: float = 0.2) -> Tuple[List[Dict], List[Dict], int]:\n",
        "        \"\"\"\n",
        "        Flatten SQuAD data into train and validation sets.\n",
        "\n",
        "        Args:\n",
        "            data (dict): The loaded SQuAD JSON data.\n",
        "            split (float): Proportion of data to use for validation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[List[Dict], List[Dict], int]: Flattened train and validation data, and error count.\n",
        "        \"\"\"\n",
        "        errors = 0\n",
        "        flattened_data_train = []\n",
        "        flattened_data_val = []\n",
        "\n",
        "        total_data = len(data.get('data', []))\n",
        "        train_range = int(total_data * (1 - split))\n",
        "\n",
        "        for i, article in enumerate(data.get(\"data\", [])):\n",
        "            title = article.get(\"title\", \"\").strip()\n",
        "            for paragraph in article.get(\"paragraphs\", []):\n",
        "                context = paragraph.get(\"context\", \"\").strip()\n",
        "                for qa in paragraph.get(\"qas\", []):\n",
        "                    try:\n",
        "                        question = qa.get(\"question\", \"\").strip()\n",
        "                        id_ = qa.get(\"id\", \"\")\n",
        "\n",
        "                        answer_starts = [answer[\"answer_start\"] for answer in qa.get(\"answers\", [])]\n",
        "                        answers = [answer[\"text\"].strip() for answer in qa.get(\"answers\", [])]\n",
        "\n",
        "                        flattened_entry = {\n",
        "                            \"title\": title,\n",
        "                            \"context\": context,\n",
        "                            \"question\": question,\n",
        "                            \"id\": id_,\n",
        "                            \"answers\": {\n",
        "                                \"answer_start\": answer_starts,\n",
        "                                \"text\": answers\n",
        "                            }\n",
        "                        }\n",
        "\n",
        "                        if i < train_range:\n",
        "                            flattened_data_train.append(flattened_entry)\n",
        "                        else:\n",
        "                            flattened_data_val.append(flattened_entry)\n",
        "                    except Exception as e:\n",
        "                        errors += 1\n",
        "                        print(f\"Error processing QA entry: {e}\")\n",
        "\n",
        "        return flattened_data_train, flattened_data_val, errors\n",
        "\n",
        "    def _save_json(self, file_path: Path, data: Dict) -> None:\n",
        "        \"\"\"\n",
        "        Save data to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to save the JSON file.\n",
        "            data (Dict): Data to save.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with file_path.open('w', encoding='utf-8') as f:\n",
        "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "            print(f\"Saved file at: {file_path}\")\n",
        "        except Exception as e:\n",
        "            raise IOError(f\"Error saving JSON file to {file_path}: {e}\")\n"
      ],
      "metadata": {
        "id": "37k518s6qW3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqfhGREGqg1r",
        "outputId": "9a35e951-c096-48c7-bb34-f82c29e8603c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "test_data = load_dataset('json', data_files='/content/alberta_squad/test_set.json', field='data')"
      ],
      "metadata": {
        "id": "t2BRfs_DqZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def get_text(answer: list) -> str:\n",
        "    \"\"\"Extract only the text from the answers.text column\n",
        "\n",
        "    Args:\n",
        "        answer: the answer.\n",
        "    \"\"\"\n",
        "    return answer[0]\n",
        "\n",
        "def get_json_data(json_path: str) -> dict:\n",
        "    \"\"\"Get the json data in form of a dictionary\n",
        "\n",
        "    Args:\n",
        "        json_path: path to the json file.\n",
        "    \"\"\"\n",
        "    # Opening JSON file\n",
        "    f = open('/content/alberta_squad/test_set.json')\n",
        "    # returns JSON object as a dictionary\n",
        "    json_data = json.load(f)\n",
        "    # Closing file\n",
        "    f.close()\n",
        "    return json_data"
      ],
      "metadata": {
        "id": "ZcCJ3lvDrAUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "def postprocess_qa_predictions(examples: datasets.arrow_dataset.Dataset,\n",
        "                               features: datasets.arrow_dataset.Dataset,\n",
        "                               raw_predictions: tuple,\n",
        "                               n_best_size: int = 20,\n",
        "                               max_answer_length: int = 50) -> collections.OrderedDict:\n",
        "    \"\"\"Function used to select the best answer from the raw predictions\n",
        "\n",
        "      Args:\n",
        "        examples: Squad samples\n",
        "        features: Squad features\n",
        "        raw_predictions: model predictions\n",
        "    \"\"\"\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "CYcLq44csbid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "def postprocess_qa_predictions(examples: datasets.arrow_dataset.Dataset,features: datasets.arrow_dataset.Dataset,\n",
        "                               raw_predictions: tuple, n_best_size: int = 20, max_answer_length: int = 50) -> collections.OrderedDict:\n",
        "\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer\n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Q702qm_xsmrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get final predictions\n",
        "with torch.no_grad():\n",
        "    pred = trainer.predict(test_features)\n"
      ],
      "metadata": {
        "id": "7Xr6EVvrs6U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test set\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"squad\")\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\n",
        "final_predictions = postprocess_qa_predictions(test_data[\"validation\"], test_features, pred.predictions, tokenizer)\n",
        "\n",
        "# Prepare predictions and references for evaluation\n",
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v[0]} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in test_data[\"validation\"]]\n",
        "\n",
        "# Compute EM and F1 scores\n",
        "result = metric.compute(predictions=formatted_predictions, references=references)\n",
        "print(f\"Exact Match (EM): {result['exact_match']}, F1 Score: {result['f1']}\")"
      ],
      "metadata": {
        "id": "K47sc6jfsnb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'exact_match': 74.07,    'f1': 80.87"
      ],
      "metadata": {
        "id": "IIr-t-6ouKxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.set_format(type=test_features.format[\"type\"], columns=list(test_features.features.keys()))\n",
        "\n",
        "# To get the final predictions we can apply our post-processing function to our raw predictions\n",
        "final_predictions = postprocess_qa_5predictions(test_data['train'], test_features, pred.predictions)\n",
        "\n",
        "formatted_5predictions = {k : v for k, v in final_predictions.items()}\n",
        "\n",
        "# Hide again the columns that are not used by the model\n",
        "test_features.set_format(type=test_features.format[\"type\"], columns=['attention_mask', 'end_positions', 'input_ids', 'start_positions'])"
      ],
      "metadata": {
        "id": "Fn5lKkQ_uBb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze top-5 predictions\n",
        "top_5_count = 0\n",
        "for ex in test_data[\"validation\"]:\n",
        "    if any(ans[\"text\"] in final_predictions[ex[\"id\"]] for ans in ex[\"answers\"][\"text\"]):\n",
        "        top_5_count += 1\n",
        "\n",
        "top_5_em = top_5_count / len(test_data[\"validation\"])\n",
        "print(f\"Top-5 Exact Match (EM): {top_5_em}\")"
      ],
      "metadata": {
        "id": "pFUZjfalsyGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 1 exact match and F1 score: {'exact_match': 75.81, 'f1': 84.478}\n",
        "Top 5 exact match: 0.8222327341532639"
      ],
      "metadata": {
        "id": "nrNv8yD6t1w_"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f4789a1f0a940af98fbf420e647d91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c7ab0f46c654403b12022317a47b11d",
              "IPY_MODEL_512ed9a66608465cacd6827532ca3f43",
              "IPY_MODEL_a103157392d9444f898ea68b9122ebcb"
            ],
            "layout": "IPY_MODEL_8e04da1d938448e685d3f0dc046b9e10"
          }
        },
        "0c7ab0f46c654403b12022317a47b11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd065203118a4fe894ba3443bf0854ea",
            "placeholder": "​",
            "style": "IPY_MODEL_c84cf4eacfcf4bf39e3eb8aafb7df1db",
            "value": "Map: 100%"
          }
        },
        "512ed9a66608465cacd6827532ca3f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830ff35654b04445a3f4fb6bb1a28175",
            "max": 87599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66ab855b429845898345a0c7d1802c8e",
            "value": 87599
          }
        },
        "a103157392d9444f898ea68b9122ebcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c69993746bc34d26a411ada9f1702d19",
            "placeholder": "​",
            "style": "IPY_MODEL_baab901383984bc89acb6a9d963c53b1",
            "value": " 87599/87599 [00:10&lt;00:00, 8928.97 examples/s]"
          }
        },
        "8e04da1d938448e685d3f0dc046b9e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd065203118a4fe894ba3443bf0854ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84cf4eacfcf4bf39e3eb8aafb7df1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "830ff35654b04445a3f4fb6bb1a28175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ab855b429845898345a0c7d1802c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c69993746bc34d26a411ada9f1702d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baab901383984bc89acb6a9d963c53b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c466844da02540b1a1b8074f51c5aa09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_493eaf44b38a48e1b2b9a92cc82c859b",
              "IPY_MODEL_f4ab6be0ac5d4a22953742e0cc9d82f9",
              "IPY_MODEL_0ca2b0def97744bdaf9b9fe1e96e154f"
            ],
            "layout": "IPY_MODEL_1a93e2562b2441589881f544fc620bef"
          }
        },
        "493eaf44b38a48e1b2b9a92cc82c859b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c34fe51b03a4f2582684e0470f13fc0",
            "placeholder": "​",
            "style": "IPY_MODEL_cfacdf1c2c264466926d5119b35abd93",
            "value": "Map: 100%"
          }
        },
        "f4ab6be0ac5d4a22953742e0cc9d82f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce5d03f5bab4b19a3fda9fb0e3bbbe1",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3528dc5228f4c589e8701c7c1b525e2",
            "value": 10570
          }
        },
        "0ca2b0def97744bdaf9b9fe1e96e154f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac3a1bd28f9441e7aebc66b9dca6d8e4",
            "placeholder": "​",
            "style": "IPY_MODEL_397c9bb5b70e474d8c24ddb6414314c6",
            "value": " 10570/10570 [00:01&lt;00:00, 8814.10 examples/s]"
          }
        },
        "1a93e2562b2441589881f544fc620bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c34fe51b03a4f2582684e0470f13fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfacdf1c2c264466926d5119b35abd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bce5d03f5bab4b19a3fda9fb0e3bbbe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3528dc5228f4c589e8701c7c1b525e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac3a1bd28f9441e7aebc66b9dca6d8e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "397c9bb5b70e474d8c24ddb6414314c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}